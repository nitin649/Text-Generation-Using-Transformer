# Text-Generation-Using-Transformers

# Description:

## Text Generation
Text generation works by utilizing algorithms and language models to process input data and generate output text, it works as an autoregressive model.


# Project Structure

1. For the purpose of this project,I have used online dataset having text related to William Shakespeare books. 

## Model Aerchitecture 

### Based on Transformer Architecture

### Self Attention
Self Attention, also called intra Attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation 
of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.

### The Decoder Block
The Decoder block is an essential component of the Transformer model that generates output sequences by interpreting encoded input sequences processed by the Encoder block. 
It is composed of five distinct parts.
1. Positional Encoder
2. Masked Multi-Head Self-Attention Layer
3. Encoder-Decoder Attention Layer
4. Feed-Forward network
5. Final Linear layer
. 

   
