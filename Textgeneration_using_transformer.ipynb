{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitin649/Text-Generation-Using-Transformers/blob/main/Textgeneration_using_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHEY5Xqowh5a",
        "outputId": "b0f566ff-87e5-4e8c-b46c-036d0d350de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading"
      ],
      "metadata": {
        "id": "WnYetzuXogzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "#read it in to inspect it\n",
        "with open('/content/drive/MyDrive/datasets/shakespeare.txt', 'r',encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(text[:200]) #will print first 200 characters"
      ],
      "metadata": {
        "id": "IgjpgDpbw32W",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print length of entire date\n",
        "print('len is {}'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O53-nFbvxafL",
        "outputId": "6ed4df33-d236-4e42-ebcd-fc24a7049401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len is 1115393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating index to character and character to index mapping"
      ],
      "metadata": {
        "id": "3ECHBW9Pomoy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating mapping from index to characters and characters to index\n",
        "chars = sorted(set(list(text))) #making set of all characters and sorting them.\n",
        "vocab_size = len(chars)\n",
        "print('all characters together -->',''.join(chars))\n",
        "print('vocab size --->',vocab_size)\n",
        "\n",
        "stoi = {char: i for i , char in enumerate(chars)}\n",
        "itos = {i : char for i , char in enumerate(chars)}\n",
        "encode = lambda string  : [stoi[char] for char in string] #take a string , output list of intergers\n",
        "decode = lambda list_index : ''.join([itos[index] for index in list_index]) #take list of index and gives a string\n",
        "\n",
        "print(encode(\"how are you\"))\n",
        "print(decode(encode(\"how are you\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLZfO-rsxlGA",
        "outputId": "1f7fb15a-9793-44f1-a9be-73001b0c5584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all characters together --> \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size ---> 65\n",
            "[46, 53, 61, 1, 39, 56, 43, 1, 63, 53, 59]\n",
            "how are you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#encode entire data into pytorch tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b55FjuEdyCeG",
        "outputId": "f74a0439-fada-4b04-9129-874bb03eb7cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115393]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "#note we are taking entire data as a single string.\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "6U0dbAFQ0fT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDQPUt-X0njc",
        "outputId": "5b9f80d0-8ca6-4415-a2ea-09fd855d3c99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now this will be an autoregressive model and auto-regressive model works as the following.\n",
        "#my -->name\n",
        "#my + name --> xyz ....\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target = y[t]\n",
        "  print('when input is {} the target {}'.format(context , target))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IOYpuh20o8Z",
        "outputId": "35088c83-f2a1-46f2-8f2b-a5e8690347cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target 47\n",
            "when input is tensor([18, 47]) the target 56\n",
            "when input is tensor([18, 47, 56]) the target 57\n",
            "when input is tensor([18, 47, 56, 57]) the target 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Index\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    print(ix)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) #stack will stackup all datapoints together to create a single batch\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ0nwZex1lfN",
        "outputId": "71484f71-4770-4e53-8239-0f11c542cff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 77170, 236647, 935582, 561895])\n",
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[53, 59,  6,  1, 58, 56, 47, 40],\n",
            "        [49, 43, 43, 54,  1, 47, 58,  1],\n",
            "        [13, 52, 45, 43, 50, 53,  8,  0],\n",
            "        [ 1, 39,  1, 46, 53, 59, 57, 43]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[59,  6,  1, 58, 56, 47, 40, 59],\n",
            "        [43, 43, 54,  1, 47, 58,  1, 58],\n",
            "        [52, 45, 43, 50, 53,  8,  0, 26],\n",
            "        [39,  1, 46, 53, 59, 57, 43,  0]])\n",
            "----\n",
            "when input is [53] the target: 59\n",
            "when input is [53, 59] the target: 6\n",
            "when input is [53, 59, 6] the target: 1\n",
            "when input is [53, 59, 6, 1] the target: 58\n",
            "when input is [53, 59, 6, 1, 58] the target: 56\n",
            "when input is [53, 59, 6, 1, 58, 56] the target: 47\n",
            "when input is [53, 59, 6, 1, 58, 56, 47] the target: 40\n",
            "when input is [53, 59, 6, 1, 58, 56, 47, 40] the target: 59\n",
            "when input is [49] the target: 43\n",
            "when input is [49, 43] the target: 43\n",
            "when input is [49, 43, 43] the target: 54\n",
            "when input is [49, 43, 43, 54] the target: 1\n",
            "when input is [49, 43, 43, 54, 1] the target: 47\n",
            "when input is [49, 43, 43, 54, 1, 47] the target: 58\n",
            "when input is [49, 43, 43, 54, 1, 47, 58] the target: 1\n",
            "when input is [49, 43, 43, 54, 1, 47, 58, 1] the target: 58\n",
            "when input is [13] the target: 52\n",
            "when input is [13, 52] the target: 45\n",
            "when input is [13, 52, 45] the target: 43\n",
            "when input is [13, 52, 45, 43] the target: 50\n",
            "when input is [13, 52, 45, 43, 50] the target: 53\n",
            "when input is [13, 52, 45, 43, 50, 53] the target: 8\n",
            "when input is [13, 52, 45, 43, 50, 53, 8] the target: 0\n",
            "when input is [13, 52, 45, 43, 50, 53, 8, 0] the target: 26\n",
            "when input is [1] the target: 39\n",
            "when input is [1, 39] the target: 1\n",
            "when input is [1, 39, 1] the target: 46\n",
            "when input is [1, 39, 1, 46] the target: 53\n",
            "when input is [1, 39, 1, 46, 53] the target: 59\n",
            "when input is [1, 39, 1, 46, 53, 59] the target: 57\n",
            "when input is [1, 39, 1, 46, 53, 59, 57] the target: 43\n",
            "when input is [1, 39, 1, 46, 53, 59, 57, 43] the target: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data) - block_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01QgFBYu2Ma5",
        "outputId": "3d5938bb-d082-4f97-edf0-5385145c327d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1003845"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Bigram Model"
      ],
      "metadata": {
        "id": "xo0Z9b05o6Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size , vocab_size)\n",
        "\n",
        "    def forward(self,idx,target=None):\n",
        "\n",
        "        logits = self.token_embedding_table(idx) #(B,T,C) #batch , timestamps , embedding_size\n",
        "        if target is None:\n",
        "            loss =None\n",
        "        else:\n",
        "            B , T , C = logits.shape\n",
        "            logits = logits.view(B*T,C) #doing this so that we can feed this into loss function\n",
        "            target = target.view(B*T)\n",
        "            loss = F.cross_entropy(logits , target)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        count=0\n",
        "        for _ in range(max_new_tokens):\n",
        "            count+=1\n",
        "            #print('initial idx',idx , idx.shape)\n",
        "            logits , loss = self(idx)\n",
        "            #print('logits shape',logits.shape)\n",
        "            #focus only on last time step\n",
        "            logits = logits[:,-1,:] #(B,C)\n",
        "            probs = F.softmax(logits, dim=-1) #B ,C\n",
        "            #sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) #(B,1)\n",
        "            #print('predicted id',idx_next)\n",
        "            #append sampled index to the running sequence\n",
        "            idx = torch.cat((idx , idx_next) , dim=1) #(B,T+1)\n",
        "            # print('concated id',idx)\n",
        "            # if count == 3:\n",
        "            #     break\n",
        "        return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb , yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "id": "O3-TqOlQ2bu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "FO9AuuXeJKKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "id": "qRrIDGakNwgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea29daa-b642-4a39-dae8-cfbc493b1090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.578680038452148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEMhUY8eO_QD",
        "outputId": "b2e8bc0b-fbaa-44f2-9215-a550541bc047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "xiKi-RJ:COpVuUa!U?qMH.uk!sCuMXvv!CJFfx;LgRyJknOEti.?I&-gPlLyulId?XlaInQ'q,lT$\n",
            "3Q&sGlvHQ?mqSq-eON\n",
            "x?S\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#trained model without adding context of previous time step\n",
        "import torch\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "print('x shape is this {} '.format(x.shape))\n",
        "#we can take mean of previous time step together to get context of current time step\n",
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow= torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] #(t,c)\n",
        "        #print(xprev.shape)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n",
        "\n",
        "\n",
        "print('x' , x[0])\n",
        "xbow[0] , xbow[0].shape #this method is not efficient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0Hhf5Hc5lEy",
        "outputId": "983036c3-a39e-4e25-b6ce-faa9fed6e960"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape is this torch.Size([4, 8, 2]) \n",
            "x tensor([[ 0.1808, -0.0700],\n",
            "        [-0.3596, -0.9152],\n",
            "        [ 0.6258,  0.0255],\n",
            "        [ 0.9545,  0.0643],\n",
            "        [ 0.3612,  1.1679],\n",
            "        [-1.3499, -0.5102],\n",
            "        [ 0.2360, -0.2398],\n",
            "        [-0.9211,  1.5433]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]),\n",
              " torch.Size([8, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Working of attention mechanism using different approach"
      ],
      "metadata": {
        "id": "nbfUssDdqZZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFl_GnTj5uAu",
        "outputId": "b931cd55-7cb2-4e65-b913-b98a2b1b0fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T)) #give lower triangular matrix\n",
        "print(wei)\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "print(wei) #normalizing values so that\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "xbow2[0]"
      ],
      "metadata": {
        "id": "MBdPRHW7Pqw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecdea9e-e9bf-422c-c76c-c3f7156cc073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "#print(tril)\n",
        "wei = torch.zeros((T,T))\n",
        "#print(wei)\n",
        "wei = wei.masked_fill(tril==0,float('-inf')) #replacing zero with -inf\n",
        "#print(wei)\n",
        "wei = F.softmax(wei, dim=-1) #this\n",
        "print(wei)\n",
        "xbow3= wei @ x\n",
        "xbow3[0]\n",
        "\n",
        "# tril = torch.tril(torch.ones(T, T))\n",
        "# wei = torch.zeros((T,T))\n",
        "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "# wei = F.softmax(wei, dim=-1)\n",
        "# xbow3 = wei @ x\n",
        "# torch.allclose(xbow, xbow3)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2prD6nw5icA",
        "outputId": "cda1120e-d4c2-42b0-bcd9-7dbb6d242b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1808, -0.0700],\n",
              "        [-0.0894, -0.4926],\n",
              "        [ 0.1490, -0.3199],\n",
              "        [ 0.3504, -0.2238],\n",
              "        [ 0.3525,  0.0545],\n",
              "        [ 0.0688, -0.0396],\n",
              "        [ 0.0927, -0.0682],\n",
              "        [-0.0341,  0.1332]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#attention mechanism\n",
        "import torch\n",
        "#query , key , value\n",
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "head_size =16\n",
        "\n",
        "query = nn.Linear(C,head_size,bias=False) #(B , T , head_size)\n",
        "key = nn.Linear(C,head_size,bias=False) #(B, T ,head_size) #\n",
        "value = nn.Linear(C,head_size,bias=False)\n",
        "q = query(x)\n",
        "k = key(x)\n",
        "wei = q @ k.transpose(-2 , -1) #(B , T , 16) @ (B , 16 , T) #after transpose of key\n",
        "#print(wei.shape) #(B,T,T)\n",
        "\n",
        "#mask attention\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = wei.masked_fill(tril==0,float('-inf')) #wei matrix is product of query @ key.transpose()\n",
        "wei = F.softmax(wei,dim=-1)\n",
        "print('wei',wei[0])\n",
        "\n",
        "v = value(x)\n",
        "\n",
        "print(v[0])\n",
        "out = wei @ v\n",
        "print(out[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM56jvBLhNhv",
        "outputId": "311ab204-a47c-4285-a3a3-284d95f51573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5877, 0.4123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4457, 0.2810, 0.2733, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2220, 0.7496, 0.0175, 0.0109, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0379, 0.0124, 0.0412, 0.0630, 0.8454, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5497, 0.2187, 0.0185, 0.0239, 0.1831, 0.0062, 0.0000, 0.0000],\n",
            "        [0.2576, 0.0830, 0.0946, 0.0241, 0.1273, 0.3627, 0.0507, 0.0000],\n",
            "        [0.0499, 0.1052, 0.0302, 0.0281, 0.1980, 0.2657, 0.1755, 0.1474]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
            "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
            "        [ 0.8321, -0.8144, -0.3242,  0.5191, -0.1252, -0.4898, -0.5287, -0.0314,\n",
            "          0.1072,  0.8269,  0.8132, -0.0271,  0.4775,  0.4980, -0.1377,  1.4025],\n",
            "        [ 0.6035, -0.2500, -0.6159,  0.4068,  0.3328, -0.3910,  0.1312,  0.2172,\n",
            "         -0.1299, -0.8828,  0.1724,  0.4652, -0.4271, -0.0768, -0.2852,  1.3875],\n",
            "        [ 0.6657, -0.7096, -0.6099,  0.4348,  0.8975, -0.9298,  0.0683,  0.1863,\n",
            "          0.5400,  0.2427, -0.6923,  0.4977,  0.4850,  0.6608,  0.8767,  0.0746],\n",
            "        [ 0.1536,  1.0439,  0.8457,  0.2388,  0.3005,  1.0516,  0.7637,  0.4517,\n",
            "         -0.7426, -1.4395, -0.4941, -0.3709, -1.1819,  0.1000, -0.1806,  0.5129],\n",
            "        [-0.8920,  0.0578, -0.3350,  0.8477,  0.3876,  0.1664, -0.4587, -0.5974,\n",
            "          0.4961,  0.6548,  0.0548,  0.9468,  0.4511,  0.1200,  1.0573, -0.2257],\n",
            "        [-0.4849,  0.1655, -0.2221, -0.1345, -0.0864, -0.6628, -0.0936,  0.1050,\n",
            "         -0.2612,  0.1854,  0.3171, -0.1393,  0.5486, -0.4086, -0.3851,  0.7106],\n",
            "        [ 0.2042,  0.3772, -1.1255,  0.3995,  0.1489,  0.3590, -0.1791,  1.3732,\n",
            "          0.1588, -0.2320,  0.1651,  0.7604,  0.3521, -1.0864, -0.7939, -0.3025]],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
            "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
            "        [ 0.2507,  0.1815, -0.0388, -0.2458, -0.1356,  0.2369, -0.1588, -0.3209,\n",
            "         -0.4772,  0.4530,  0.4388, -0.3604, -0.0859, -0.0803,  0.1115,  0.9138],\n",
            "        [ 0.3288,  0.0950, -0.1875, -0.0916, -0.0079,  0.0883, -0.0678, -0.1830,\n",
            "         -0.4008,  0.0761,  0.3542, -0.1453, -0.1970, -0.0976,  0.0109,  1.0278],\n",
            "        [ 0.6067, -0.4271, -0.2246,  0.2273, -0.1100, -0.2183, -0.3709, -0.1340,\n",
            "         -0.1130,  0.6494,  0.6441, -0.1387,  0.2489,  0.2713, -0.0351,  1.2031],\n",
            "        [ 0.2010,  0.8507,  0.6533,  0.2228,  0.3173,  0.8365,  0.6526,  0.3822,\n",
            "         -0.6315, -1.2205, -0.4374, -0.2859, -0.9985,  0.1108, -0.1001,  0.5346],\n",
            "        [ 0.1453,  0.4755,  0.1447, -0.2496, -0.0209,  0.4674,  0.0808, -0.2074,\n",
            "         -0.5866,  0.0157,  0.1711, -0.3741, -0.3699, -0.1248,  0.1164,  0.7404],\n",
            "        [-0.2268,  0.2806, -0.0834,  0.2215,  0.1804,  0.2529, -0.0778, -0.2663,\n",
            "         -0.1468,  0.1037,  0.0856,  0.1898, -0.0721, -0.0397,  0.3974,  0.4161],\n",
            "        [-0.1450,  0.2375, -0.1882,  0.3479,  0.1843,  0.1369, -0.0581,  0.1339,\n",
            "         -0.0594, -0.0362,  0.0767,  0.2613,  0.0609, -0.1358,  0.0764,  0.3417]],\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main code for decoder model inlcuding all the required functions"
      ],
      "metadata": {
        "id": "YWF6DlMFpgh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete code\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 10000\n",
        "eval_interval = 200\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4 #no of blocks\n",
        "dropout = 0.0\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('/content/drive/MyDrive/datasets/shakespeare.txt', 'r',encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "#data preprocessig\n",
        "chars = sorted(set(list(text))) #making set of all characters and sorting them.\n",
        "vocab_size = len(chars)\n",
        "print('all characters together -->',''.join(chars))\n",
        "print('vocab size --->',vocab_size)\n",
        "\n",
        "stoi = {char: i for i , char in enumerate(chars)}\n",
        "itos = {i : char for i , char in enumerate(chars)}\n",
        "encode = lambda string  : [stoi[char] for char in string] #take a string , output list of intergers\n",
        "decode = lambda list_index : ''.join([itos[index] for index in list_index]) #take list of index and gives a string\n",
        "\n",
        "#train and test split\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "#data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) #this will generate a batch of randomly selected values between upper and lower limit.\n",
        "    #print(ix)\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) #stack will stackup all datapoints together to create a single batch\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x.to(device), y.to(device)\n",
        "\n",
        "class LayerNorm1d:\n",
        "    #layer normalization is same as batchnormalization , the main difference is in batch-normalization we calculate mean along the column of each batch\n",
        "    #example - [[1,2,3],[5,6,7]] = mean would be [(1+5)/2 , (2+6)/2 , (3+7)/2] so when we use mean to do normalization we get some data augmentation\n",
        "    #as we consider some context of each sample in the batch to calculate single row on the other hand in layer normalization we do this on each row\n",
        "    #means we calculate mean along the rows = mean would be [(1+2+3) / 3 , (5+6+7)/3] --> no data augementation\n",
        "    #Note here we dont need to maintain buffer for mean and variacne for testing as we calculate these values using the row only.\n",
        "    def __init__(self, dim , eps = 1e-5 ,momentum=0.1):\n",
        "        self.eps = eps\n",
        "        self.gamma = torch.ones(dim)\n",
        "        self.beta - torch.zeros(dim)\n",
        "    def __call__(self,x):\n",
        "        xmean = x.mean(1,keepdim=True) #batch mean and in batch_norm dim will be 0 (along with column)\n",
        "        xvar = x.var(1,keepdim=True) #batch variance\n",
        "        xhat = (x-xmean)/torch.sqrt(xvar+self.eps) #normalize to unit variance\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "        return self.out\n",
        "    def parameters(self):\n",
        "        return [self.gamma,self.beta] #trainable parameters\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self,head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd , head_size , bias=False)\n",
        "        self.query = nn.Linear(n_embd , head_size , bias=False)\n",
        "        self.value = nn.Linear(n_embd , head_size , bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #we use pytorch register buffer to store this tril matrix\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x):\n",
        "        B, T , C = x.shape\n",
        "        k = self.key(x)\n",
        "        q=  self.query(x)\n",
        "        #compute attention scores ('affinities')\n",
        "        scores = q @ k.transpose(-2,-1)* C**-0.5#(B,T,C) @(B,C,T) -->(B,T,T)\n",
        "        #as per the original paper formula for calculating scores is ((q @ k.transpose) / underroot(embeding_size)) @ V\n",
        "        scores = scores.masked_fill(self.tril[:T, :T]==0,float('-inf')) #(B,T,T) (self.tril[:T,;T]) means taking whole tril matrix\n",
        "        scores = F.softmax(scores,dim=-1) #(B, T, T)\n",
        "        scores = self.dropout(scores)\n",
        "        v = self.value(x) #(B,T,C)\n",
        "        out = scores @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multiple head of self-attention in parallel\"\"\"\n",
        "    def __init__(self,num_heads , head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd , n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = torch.cat([h(x) for h in self.heads],dim=-1)#we calculate head size using embedding_size // no_of_heads\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
        "    def __init__(self,n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd , 4* n_embd),#as mentioned in attention all you need paper input is = 512(embedding_size) and output is 2048 (which is nothing but 4 * 512)\n",
        "            # While the linear transformations are the same across different positions, they use different parameters\n",
        "            # from layer to layer. Another way of describing this is as two convolutions with kernel size 1.\n",
        "            # The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\n",
        "            # dff = 2048.\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4*n_embd , n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"transformer block \"\"\"\n",
        "    def __init__(self,n_embd, n_head):\n",
        "         # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "         super().__init__()\n",
        "         head_size = n_embd//n_head\n",
        "         self.sa= MultiHeadAttention(n_head, head_size)\n",
        "         self.ffwd = FeedForward(n_embd)\n",
        "         self.ln1=nn.LayerNorm1d(n_embd)\n",
        "         self.ln2=nn.LayerNorm1d(n_embd)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x + self.sa(self.ln1(x)) #residual connection ( as mentioned in paper (add + norm ) computation)\n",
        "        #now here we made some changes so as per the original paper we normalized results after getting ouput from self attention block.\n",
        "        #but nowadays in practice we usually do this normalization before feeding data into self - attention block so as for ffwd layer.\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class LanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size , n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd) #adding position embedding as per the paper but using different approach (block_size,pos_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd ,n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm1d(n_embd) #final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd , vocab_size)\n",
        "\n",
        "    def forward(self,idx ,targets=None):\n",
        "        B , T = idx.shape\n",
        "        #idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding(idx) #(B,T C)\n",
        "        pos_emb = self.position_embedding(torch.arange(T,device=device)) #(T,C)\n",
        "        x = tok_emb + pos_emb #(B,T,C)-->output shape\n",
        "        x = self.blocks(x) #(B,T,C)\n",
        "        x = self.ln_f(x) #(B,T,C)\n",
        "        logits = self.lm_head(x) #(B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T , C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss=F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:] #because we need maximum context of length 32 we are fixing -32 size\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n"
      ],
      "metadata": {
        "id": "CAHgbTERhS9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ab46af-2f00-4a67-86c6-fd2f4bd97edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all characters together --> \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size ---> 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz2NjV9S9XUt",
        "outputId": "da66a604-8627-4535-c3bb-84c7abf3e2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4112, val loss 4.4015\n",
            "step 200: train loss 2.5096, val loss 2.5115\n",
            "step 400: train loss 2.3491, val loss 2.3596\n",
            "step 600: train loss 2.2455, val loss 2.2547\n",
            "step 800: train loss 2.1681, val loss 2.1723\n",
            "step 1000: train loss 2.1014, val loss 2.1358\n",
            "step 1200: train loss 2.0320, val loss 2.0732\n",
            "step 1400: train loss 1.9968, val loss 2.0533\n",
            "step 1600: train loss 1.9544, val loss 2.0325\n",
            "step 1800: train loss 1.9264, val loss 2.0002\n",
            "step 2000: train loss 1.8799, val loss 1.9729\n",
            "step 2200: train loss 1.8821, val loss 1.9658\n",
            "step 2400: train loss 1.8419, val loss 1.9523\n",
            "step 2600: train loss 1.8179, val loss 1.9358\n",
            "step 2800: train loss 1.8011, val loss 1.9365\n",
            "step 3000: train loss 1.7893, val loss 1.9383\n",
            "step 3200: train loss 1.7699, val loss 1.9085\n",
            "step 3400: train loss 1.7557, val loss 1.8833\n",
            "step 3600: train loss 1.7468, val loss 1.8764\n",
            "step 3800: train loss 1.7245, val loss 1.8660\n",
            "step 4000: train loss 1.7244, val loss 1.8575\n",
            "step 4200: train loss 1.7093, val loss 1.8597\n",
            "step 4400: train loss 1.7097, val loss 1.8503\n",
            "step 4600: train loss 1.7091, val loss 1.8534\n",
            "step 4800: train loss 1.6797, val loss 1.8487\n",
            "step 5000: train loss 1.6728, val loss 1.8277\n",
            "step 5200: train loss 1.6699, val loss 1.8295\n",
            "step 5400: train loss 1.6678, val loss 1.8226\n",
            "step 5600: train loss 1.6522, val loss 1.7991\n",
            "step 5800: train loss 1.6570, val loss 1.8065\n",
            "step 6000: train loss 1.6463, val loss 1.8159\n",
            "step 6200: train loss 1.6345, val loss 1.7824\n",
            "step 6400: train loss 1.6330, val loss 1.7854\n",
            "step 6600: train loss 1.6282, val loss 1.7877\n",
            "step 6800: train loss 1.6223, val loss 1.7743\n",
            "step 7000: train loss 1.6287, val loss 1.7884\n",
            "step 7200: train loss 1.6151, val loss 1.7780\n",
            "step 7400: train loss 1.6120, val loss 1.7566\n",
            "step 7600: train loss 1.6188, val loss 1.7505\n",
            "step 7800: train loss 1.6069, val loss 1.7655\n",
            "step 8000: train loss 1.6010, val loss 1.7742\n",
            "step 8200: train loss 1.5954, val loss 1.7683\n",
            "step 8400: train loss 1.6009, val loss 1.7668\n",
            "step 8600: train loss 1.5948, val loss 1.7615\n",
            "step 8800: train loss 1.5827, val loss 1.7513\n",
            "step 9000: train loss 1.5820, val loss 1.7621\n",
            "step 9200: train loss 1.5793, val loss 1.7495\n",
            "step 9400: train loss 1.5775, val loss 1.7511\n",
            "step 9600: train loss 1.5737, val loss 1.7630\n",
            "step 9800: train loss 1.5791, val loss 1.7460\n",
            "step 9999: train loss 1.5704, val loss 1.7489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Data from model"
      ],
      "metadata": {
        "id": "jSoDNOxNpZ3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G91UxGaN9wzN",
        "outputId": "7cc72d52-b054-4b85-fec2-8a8557177dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "And they bride with to law be mades;\n",
            "Thou but take Ond my call'd heart backen'd him vet?\n",
            "Feetle was, away, my facal'nce zolous\n",
            "Yours, to fignic me mildnicling effireeds, heir latisna,\n",
            "Wove to hear me now on that spelfling me lie-huse coursent the bains\n",
            "Withy holven'd norforewell? good thy world that\n",
            "more-nought their king at too rive\n",
            "Against he poor of his burder of thrust for treary to shall I know on,\n",
            "To fight? go hometh from and toobly,\n",
            "And say you are stording me any and\n",
            "Hir-his chan your hartiments, been you shall have hanso,\n",
            "Befaning-ennought we men.\n",
            "\n",
            "CORIOLANUS:\n",
            "Where not usibun, with confessy.\n",
            "Which might to England marcians.\n",
            "\n",
            "LADY PEY:\n",
            "Well, to madam?\n",
            "\n",
            "LANCIO:\n",
            "I meet my see: in doublord my lades:\n",
            "What's my scept is gone deceed, is xugge.\n",
            "\n",
            "NORFONTEL:\n",
            "What come, we messel. Do yearn'd, do so lad not a vysedchanged;\n",
            "And detide by the nothing Captuless to doom, iuGo,\n",
            "To not the summore protentess-bent, and, bruilts, were be thou said;\n",
            "For for my watch time wout in forcesy,\n",
            "In give we road, brook hears, I'll she dear when the varcal prisofsanced,\n",
            "On backe or land's not resvemings so lears\n",
            "Singin Grue of them Warwards now,\n",
            "And friar'd abhing wInham, a caning confacest.--an Pink to do withat my brancess\n",
            "For witness what she woed, is that sticidentles where for lovels.\n",
            "Let meofing footh back on of through? Offeries mades love is captin.\n",
            "\n",
            "Provount:\n",
            "Slew you bed the been seem cannobles.\n",
            "\n",
            "GLOUM:\n",
            "And that swear; was he and unto may's froes\n",
            "To marright instain of Romear wholess eye\n",
            "Shall books thou of ploce, say, as If the ding avires feer\n",
            "Of the veer in unbanderling strangbrent\n",
            "Blady, in shall Lory.\n",
            "\n",
            "Third SON:\n",
            "\n",
            "EDWIFFORD:\n",
            "Through. I will was I was that been\n",
            "As her, byors in't; still take that lay not,\n",
            "There wife it breats are Edmiled voice, of mongl'd off?\n",
            "And cause here with fairel o' the rewburses honeroner'light, 'two'st did out are ward's known of an a would prozent. I was not trmaight, but comfor\n",
            "on the Riparforit bring shows'd for and\n",
            "As shape he had nightar\n",
            "Marr\n"
          ]
        }
      ]
    }
  ]
}